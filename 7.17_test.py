import tensorflow as tfimport osimport imageioimport numpy as npos.environ["CUDA_VISIBLE_DEVICES"] = '0'gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))def read_data(file_name):    imageList = []    labelList = []        if file_name == "Training":        name1 = "../../../../Fruit-Images-Dataset-master/"        name = os.listdir(name1 + file_name + "/")        count = len(name)                #print(name)        for i in range(count):            listdir = os.listdir(name1 + file_name + "/" + name[i])            count2 = len(listdir)                        # labelList.append                        for j in range(count2):                img = imageio.imread(                                     name1                                     + file_name + "/" + name[i] + "/" + listdir[j])                                     #img = np.array(img)                                     img = img / 255.0                                     imageList.append(img)                                                                          # labelList.append(a) was here                                     a = np.zeros(33)                                     a[i] = 1                                     labelList.append(a)print(len(imageList))    print(len(labelList))        return imageList, labelListdef compute_accuracy(v_xs, v_ys):    global prediction    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(v_ys, 1))    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})    return resultdef weights_variable(shape):    initial = tf.truncated_normal(shape, stddev=0.05, dtype=tf.float32)    return tf.Variable(initial)def biases_variable(shape):    initial = tf.constant(0.05, shape=shape)    return tf.Variable(initial)def conv2d(x, W):    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding="SAME")def max_pooling_2x2(x):    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME")# ================================================= Neural Network Starts ============================================ #xs = tf.placeholder(tf.float32, [None, 100, 100, 3]) / 255.ys = tf.placeholder(tf.float32, [None, 33])keep_prob = tf.placeholder(tf.float32)# convolutional layer 1conv1_weights = weights_variable([5, 5, 3, 16])conv1_biases = biases_variable([16])conv1_h = tf.nn.relu(conv2d(xs, conv1_weights) + conv1_biases)  # output size: 100 * 100 * 16conv1_h_pooling = max_pooling_2x2(conv1_h)  # output size: 50 * 50 * 16# convolutional layer 2conv2_weights = weights_variable([5, 5, 16, 32])conv2_biases = biases_variable([32])conv2_h = tf.nn.relu(conv2d(conv1_h_pooling, conv2_weights) + conv2_biases)  # output size: 50 * 50 * 32conv2_h_pooling = max_pooling_2x2(conv2_h)  # output size: 25 * 25 * 32# convolutional layer 3conv3_weights = weights_variable([5, 5, 32, 64])conv3_biases = biases_variable([64])conv3_h = tf.nn.relu(conv2d(conv2_h_pooling, conv3_weights) + conv3_biases)  # output size: 25 * 25 * 64conv3_h_pooling = max_pooling_2x2(conv3_h)  # output size: 13 * 13 * 64'''    # convolutional layer 4    conv4_weight = weights_variable([5, 5, 64, 128])    conv4_biases = biases_variable([128])    conv4_h = tf.nn.relu(conv2d(conv3_h_pooling, conv4_weight) + conv4_biases)  # output size: 13 * 13 * 128    conv4_h_pooling = max_pooling_2x2(conv4_h)  # output size: 7 * 7 * 128    '''# fully connected layer 1fc1_weights = weights_variable([13 * 13 * 64, 1024])fc1_biases = biases_variable([1024])h4_pooling_flat = tf.reshape(conv3_h_pooling, [-1, 13 * 13 * 64])fc1_h = tf.nn.relu(tf.matmul(h4_pooling_flat, fc1_weights) + fc1_biases)fc1_h_dropout = tf.nn.dropout(fc1_h, keep_prob)# fully connected layer 2fc2_weights = weights_variable([1024, 33])fc2_biases = biases_variable([33])prediction = tf.nn.softmax(tf.matmul(fc1_h_dropout, fc2_weights) + fc2_biases)# ================================================= Neural Network Ends ============================================== #cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(tf.clip_by_value(prediction, 1e-7, 1.0)), reduction_indices=[1]))train_step = tf.train.AdamOptimizer(1e-7).minimize(cross_entropy)init = tf.global_variables_initializer()sess.run(init)train_image, train_label = read_data("Training")indexlist = np.arange(len(train_image), dtype=np.dtype(int))np.random.shuffle(indexlist)batch_size = 50for i in range(20000):    a = i % len(train_image)    batch_xs = np.array(train_image)[indexlist[a * batch_size: (a + 1) * batch_size]]    batch_ys = np.array(train_label)[indexlist[a * batch_size: (a + 1) * batch_size]]    __, pred = sess.run([train_step, prediction], feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.8})        print(i, compute_accuracy(batch_xs,batch_ys))